# ğŸ–ï¸ **Project Plan: Real-Time Hand Gesture Recognition for Video Call Reactions**

### *(Hybrid Approach: Heuristics â†’ Streamlit UI â†’ Extended Gestures â†’ ML)*

---

## **1. Project Summary**

This project builds a **real-time gesture recognition system** that detects common hand signalsâ€”**thumbs up, thumbs down, raised hand, and clapping**â€”using webcam video. It overlays corresponding **Zoom-style reactions** (ğŸ‘ ğŸ‘ ğŸ‘ âœ‹) on the live feed.

The system uses **MediaPipe Hands** for 21-point hand landmark detection with a **five-phase approach**:

1. **Phase 1 (Complete):** Heuristic-based detection for 4 core gestures using geometric rules
2. **Phase 2 (Planned):** Streamlit UI for interactive web-based demo
3. **Phase 3 (Planned):** Extended gesture vocabulary with heuristics (finger counting, âœŒï¸ ğŸ‘Œ ğŸ‘† ğŸ‘‹ âœŠ)
4. **Phase 4 (Planned):** Data collection tool for building ML training dataset
5. **Phase 5 (Planned):** ML classifier training and integration

The project demonstrates:

* Real-time CV processing
* Rule-based gesture recognition using geometric heuristics
* Interactive web deployment (Streamlit)
* Dataset collection & labeling
* Model training + evaluation

---

## **2. Supported Gestures**

### **Core Gestures (Phase 1) âœ…**

| Gesture     | Emoji | Description                                  |
| ----------- | ----- | -------------------------------------------- |
| Thumbs Up   | ğŸ‘    | Thumb extended upward, other fingers curled  |
| Thumbs Down | ğŸ‘    | Thumb extended downward, others curled       |
| Raised Hand | âœ‹    | All fingers extended, palm upright           |
| Clapping    | ğŸ‘    | Two hands close together                     |

### **Extended Gestures (Phase 3) â€” Toggleable**

| Gesture       | Emoji   | Description                              | Default |
| ------------- | ------- | ---------------------------------------- | ------- |
| Finger Count  | 1ï¸âƒ£-5ï¸âƒ£   | Count of extended fingers                | OFF     |
| Peace Sign    | âœŒï¸      | Index + middle extended, others curled   | OFF     |
| OK Sign       | ğŸ‘Œ      | Thumb-index circle, others extended      | OFF     |
| Pointing      | ğŸ‘†      | Only index finger extended               | OFF     |
| Fist          | âœŠ      | All fingers curled, thumb tucked         | OFF     |
| Rock On       | ğŸ¤˜      | Index + pinky extended, others curled    | OFF     |

---

## **3. Phase 1: Heuristic-Based Detection (Complete)**

The initial implementation uses **geometric heuristics** on MediaPipe landmarks to detect the 4 core gestures. This approach works well because these gestures are geometrically distinct.

### **Detection Rules**

| Gesture | Heuristic Logic |
|---------|-----------------|
| ğŸ‘ Thumbs Up | Thumb tip significantly above thumb MCP + all other fingers curled (tips below MCPs) |
| ğŸ‘ Thumbs Down | Thumb tip significantly below thumb MCP + all other fingers curled |
| âœ‹ Raised Hand | All 5 fingers extended (tips above PIPs) |
| ğŸ‘ Clapping | Two hands detected + palm centers within threshold distance |

### **Why Heuristics Work for Core Gestures**

* **Geometrically distinct poses** â€” Each gesture has a unique spatial configuration
* **Low ambiguity** â€” Thumbs up/down differ only in vertical thumb orientation
* **Robust landmarks** â€” MediaPipe provides reliable 21-point tracking
* **Zero training data** â€” Works immediately without data collection
* **Fast inference** â€” Simple coordinate comparisons, no model overhead

### **Current Pipeline**

1. Capture webcam frame
2. Detect hands â†’ extract 21 landmarks per hand
3. Apply geometric rules to classify gesture
4. Temporal smoothing (majority vote over 7 frames)
5. Gesture â†’ emoji mapping
6. Display overlay

---

## **4. Phase 2: Streamlit UI (Planned)**

Build an interactive web-based interface using Streamlit for a polished demo experience.

### **UI Features**

| Feature | Description |
|---------|-------------|
| **Live webcam feed** | Real-time video with gesture overlay |
| **Gesture toggle panel** | Enable/disable individual gestures |
| **Reaction history** | Scrollable log of detected gestures with timestamps |
| **Debug mode toggle** | Show/hide hand landmarks |
| **Statistics dashboard** | Gesture counts, detection rate, FPS |
| **Settings panel** | Adjust detection sensitivity, smoothing window |

### **Why Streamlit?**

* **Rapid prototyping** â€” Build interactive UIs in pure Python
* **Built-in webcam support** â€” `st.camera_input()` for easy video capture
* **Clean aesthetics** â€” Modern, professional look out of the box
* **Easy deployment** â€” One-click deploy to Streamlit Cloud
* **Portfolio-ready** â€” Shareable link for applications/interviews

### **Technical Approach**

```
streamlit run app.py
```

* Use `st.sidebar` for controls and settings
* `st.empty()` containers for real-time video updates
* Session state for tracking gesture history and stats

---

## **5. Phase 3: Extended Gesture Vocabulary (Planned)**

Expand the gesture library using heuristic rules, with **toggle controls** to enable/disable each gesture.

### **New Gestures (Heuristic-Based)**

| Gesture | Emoji | Heuristic Logic | Toggle Default |
|---------|-------|-----------------|----------------|
| **Finger Count (1-5)** | 1ï¸âƒ£-5ï¸âƒ£ | Count extended fingers | OFF |
| **Peace Sign** | âœŒï¸ | Index + middle extended, others curled | OFF |
| **OK Sign** | ğŸ‘Œ | Thumb tip close to index tip, other fingers extended | OFF |
| **Pointing** | ğŸ‘† | Only index extended | OFF |
| **Fist** | âœŠ | All fingers curled, thumb tucked | OFF |
| **Rock On** | ğŸ¤˜ | Index + pinky extended, others curled | OFF |

### **Finger Counting Logic**

```python
def count_extended_fingers(hand_landmarks):
    """Count how many fingers are extended (0-5)."""
    count = 0
    
    # Thumb: check horizontal distance from palm
    if is_thumb_extended(hand_landmarks):
        count += 1
    
    # Fingers: tip above PIP joint = extended
    for finger in [INDEX, MIDDLE, RING, PINKY]:
        if finger_tip.y < finger_pip.y:
            count += 1
    
    return count
```

### **Toggle System**

Users can enable/disable gestures via the Streamlit UI:

* **Core gestures** (ğŸ‘ ğŸ‘ âœ‹ ğŸ‘) â€” Always ON by default
* **Extended gestures** (âœŒï¸ ğŸ‘Œ ğŸ‘† âœŠ ğŸ¤˜) â€” OFF by default, toggle ON as needed
* **Finger counting** â€” OFF by default (can conflict with other gestures)

This prevents gesture conflicts and lets users customize for their use case.

### **Observed Limitations: Why ML Is Needed**

After implementing extended gestures with heuristics, we observed significant detection conflicts that validate the need for ML:

| Conflict | Gestures Affected | Root Cause |
|----------|-------------------|------------|
| **Extended fingers overlap** | âœŒï¸ Peace vs âœ‹ Raised Hand | Both have index + middle extended; peace requires detecting ring/pinky curl precisely |
| **Single finger ambiguity** | ğŸ‘† Pointing vs ğŸ‘ Thumbs Up | Both involve one extended digit with others curled; thumb position is subtle |
| **Curled fingers similarity** | âœŠ Fist vs base of thumbs gestures | All have curled fingers; only thumb orientation differs |
| **Counting conflicts** | ğŸ”¢ Finger count vs named gestures | 5 fingers = raised hand, 2 fingers = peace, 1 finger = pointing |
| **Orientation sensitivity** | All gestures | Heuristics assume upright hand; rotated hands break thresholds |

**Key Insight:** Heuristics excel at **geometrically distinct** poses (thumbs up vs open palm) but fail when gestures share structural similarities. The rules become increasingly brittle as more gestures are added.

**ML Advantages:**
* Learns subtle differences from real examples rather than hand-coded thresholds
* Handles natural variation in how different people perform gestures
* Provides confidence scores to handle ambiguous cases gracefully
* Generalizes across hand orientations and distances

This observation motivates **Phase 4 (Data Collection)** and **Phase 5 (ML Training)** to achieve production-quality recognition for the full gesture vocabulary.

---

## **6. Phase 4: Data Collection Tool (Planned)**

Build a dedicated tool to collect and manage video data for ML training.

### **Data Collection App Features**

| Feature | Description |
|---------|-------------|
| **Recording interface** | Live webcam with start/stop recording |
| **Gesture label selection** | Dropdown or hotkeys to tag current gesture |
| **Session management** | Name sessions, track participants |
| **Progress tracker** | Show samples collected per gesture class |
| **Data preview** | Review recorded clips before saving |
| **Export options** | Save as CSV (landmarks) or video clips |

### **Data Organization**

```
data/
â”œâ”€â”€ sessions/
â”‚   â”œâ”€â”€ session_001_alice/
â”‚   â”‚   â”œâ”€â”€ metadata.json       # participant info, date, settings
â”‚   â”‚   â”œâ”€â”€ thumbs_up/
â”‚   â”‚   â”‚   â”œâ”€â”€ clip_001.csv    # landmark coordinates per frame
â”‚   â”‚   â”‚   â”œâ”€â”€ clip_002.csv
â”‚   â”‚   â”œâ”€â”€ peace_sign/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ session_002_bob/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ combined/
â”‚   â””â”€â”€ all_landmarks.csv       # merged dataset for training
â””â”€â”€ stats.json                  # collection progress summary
```

### **Metadata Tracking**

```json
{
  "session_id": "session_001",
  "participant": "Alice",
  "date": "2024-01-15",
  "duration_minutes": 12,
  "samples_collected": {
    "thumbs_up": 156,
    "thumbs_down": 142,
    "peace_sign": 98,
    "...": "..."
  },
  "lighting": "natural",
  "distance": "medium",
  "hand": "right"
}
```

### **Collection Guidelines**

* **Target:** 100-300 samples per gesture per participant
* **Variation:** Multiple distances, lighting conditions, both hands
* **Quality:** Auto-skip frames where hand detection fails

---

## **7. Phase 5: ML Classifier (Planned)**

Train and integrate a machine learning classifier for robust gesture recognition.

### **Why Add ML After Heuristics?**

| Limitation of Heuristics | ML Solution |
|--------------------------|-------------|
| **Orientation sensitivity** â€” Rules assume upright hand | ML generalizes across orientations |
| **Inter-user variability** â€” Fixed thresholds don't fit everyone | ML adapts to population variation |
| **Edge cases** â€” Hard to write rules for ambiguous poses | ML learns from examples |
| **Confidence scores** â€” Heuristics are binary yes/no | ML provides probabilities |

### **Model Selection Decisions**

#### **Primary: Random Forest** âœ… Recommended

| Aspect | Details |
|--------|---------|
| **Why** | Fast training & inference, handles multiclass natively, no hyperparameter sensitivity |
| **Training** | O(n Ã— m Ã— log n) â€” trains in seconds |
| **Inference** | O(tree_depth Ã— n_trees) â€” just tree traversals, very fast |
| **Multiclass** | Native support (no OvO/OvA strategy needed) |
| **Settings** | n_estimators=200, max_depth=20, class_weight='balanced' |

#### **Secondary: SVM with RBF Kernel**

| Aspect | Details |
|--------|---------|
| **Why** | Strong performance on small-medium datasets, good generalization |
| **Multiclass Strategy** | **One-vs-One (OvO)** â€” scikit-learn default |
| **OvO Explained** | Trains kÃ—(k-1)/2 binary classifiers (45 for 10 gesture classes) |
| **Why OvO over OvA?** | Each OvO classifier sees balanced binary data; OvA creates imbalanced problems (1 class vs ALL others) |
| **Settings** | kernel='rbf', C=1.0, probability=True |

#### **Why NOT Neural Networks?**

| Reason | Explanation |
|--------|-------------|
| **Dataset size** | Landmark data is small (1000s of samples) â€” DNNs need more |
| **Feature space** | Only 63 features (21 landmarks Ã— 3 coords) â€” not high-dimensional |
| **Interpretability** | RF/SVM easier to debug than black-box neural nets |
| **Deployment** | No GPU, PyTorch/TensorFlow dependencies needed |
| **Speed** | RF inference is microseconds; neural nets add latency |

### **Feature Space**

* **Input:** 63 normalized features (x, y, z for 21 landmarks)
* **Normalization:** Wrist-centered, unit-scaled (done during data collection)
* **No additional feature engineering** â€” landmarks are already good features

### **Pipeline (with ML)**

1. Capture webcam frame
2. Detect hands â†’ extract 21 landmarks
3. Normalize landmarks (wrist = origin, scale to unit)
4. ML classifier predicts gesture + confidence
5. Temporal smoothing + confidence thresholding
6. Display overlay

### **Integration Strategy**

* **Hybrid mode:** Use heuristics as fallback when ML confidence is low
* **A/B comparison:** Toggle between heuristic and ML modes in UI
* **Gradual rollout:** Start with ML for extended gestures only

**Backend:** Python, OpenCV, MediaPipe, scikit-learn

**Frontend:** Streamlit

---

## **8. ML Data Collection Details** *(Phase 4-5 Reference)*

Gesture classifiers built on MediaPipe landmarks need **far less data** than image-based models because the feature space is low-dimensional and highly structured.

Below is a full professional-grade data collection workflow.

---

### **8.1. Data Requirements**

To get a clean demo-quality classifier:

#### **Minimum Viable (For Demo Only)**

* **50â€“100 samples per gesture**
* Total â‰ˆ **250â€“500 samples**

#### **Robust (Recommended for Application Portfolio)**

* **300â€“500 samples per gesture**
* Across **3â€“5 different people**
* Total: **1500â€“2500 samples**

This size is easy to collect:

* 10 seconds per gesture per person
* At ~15 FPS â†’ 150 frames per gesture per person
* 3â€“4 volunteers â†’ 450â€“600 frames per gesture

---

### **8.2. Data Collection Script**

Write a Python tool that:

* Displays webcam feed with MediaPipe landmarks
* Lets user select label using keyboard keys
* Saves each frame's **normalized landmark coordinates** to a CSV

#### **Example UI Mapping**

| Key | Label       |
| --- | ----------- |
| `1` | thumbs_up   |
| `2` | thumbs_down |
| `3` | raised_hand |
| `4` | clap        |
| `0` | none        |

#### **Data Saved Per Frame**

```
x1, y1, x2, y2, â€¦, x21, y21, label
```

If including depth info:

```
x1, y1, z1, x2, y2, z2, â€¦, label
```

#### **Normalization Strategy**

Normalize per hand:

* Translate so wrist = origin
* Scale so max distance from wrist = 1
* Optional: rotate to align palm orientation

This makes training more robust across users and distances from camera.

---

### **8.3. How to Capture High-Quality Data**

#### **A. Collect "steady pose" samples**

The participant holds the gesture for 5â€“10 seconds.

Avoid:

* mid-transition frames
* frames where gesture is unclear
* occlusion

#### **B. Use multiple distances**

Ask participants to record at:

* Close-up (face distance)
* Mid-distance (upper torso)
* Far distance (full upper body)

This helps generalization.

#### **C. Collect left and right hand data**

Gesture detection should work ambidextrously.

#### **D. Vary background & lighting**

Record in:

* bright light
* dimmer light
* cluttered vs plain backgrounds

Even though landmarks abstract away pixels, this still helps reduce tracking failures.

#### **E. For clapping**

Collect **motion sequences**, not static poses:

* Record 5â€“10 seconds of clapping
* Keep all frames
* Later compute temporal differences if needed (e.g., distance between palms decreasing/increasing)

---

### **8.4. Cleaning & Preparing the Dataset**

After recording, process each CSV:

1. **Drop frames where hand detection failed**
   (Some rows will have NaNs if MediaPipe didn't detect the hand.)

2. **Remove duplicates and static frames**
   For clapping, keep frames where movement exists.

3. **Ensure even class distribution**
   If gestures have 500 samples but "none" has 100, pad "none" using extra recordings.

4. **Optionally augment the landmarks**
   * Add Gaussian noise (tiny jitter)
   * Random small rotations
   * Mirror left-hand â†” right-hand coordinates

This improves model robustness.

---

### **8.5. Model Training**

#### **Models that work well**

* **SVM (RBF kernel)**
* **RandomForest (n_estimators=200â€“300)**
* **Tiny MLP**:
  * 2â€“3 dense layers of 64â€“128 units

#### **Training Workflow**

1. Load all CSVs
2. Shuffle
3. Train/validation split: **80/20**
4. Train model
5. Evaluate on validation set:
   * Accuracy
   * Per-class recall
   * Confusion matrix

#### **Typical expected performance**

With ~300 samples/gesture:

* **Accuracy:** 90â€“98%
* **Clapping** may be lower due to motion, but network still detects the presence of two hands very well.

---

### **8.6. Saving the Model**

Use joblib:

```python
import joblib
joblib.dump(model, "gesture_classifier.pkl")
```

At runtime:

```python
model = joblib.load("gesture_classifier.pkl")
prediction = model.predict([landmark_vector])
```

---

## **9. Real-Time System**

### **Inference Pipeline**

**Phase 1-3 (Heuristics):**
1. Webcam frame â†’ MediaPipe â†’ hand landmarks
2. Apply geometric rules for gesture classification
3. Apply **temporal smoothing** (majority vote over last 7 frames)
4. Display reaction overlay

**Phase 5 (ML):**
1. Webcam frame â†’ MediaPipe â†’ hand landmarks
2. Normalize landmarks â†’ predict with classifier
3. Apply temporal smoothing + confidence thresholding
4. Display reaction overlay

### **Overlays**

* Emoji floats up & fades
* Reaction log in sidebar ("ğŸ‘ detected at 12:36:22")
* Hand skeleton rendering for debugging (toggle with 'd' key)

---

## **10. Timeline**

### **Phase 1: Core Heuristics (Complete) â€” 3 Days**

| Day | Tasks |
|-----|-------|
| 1 | Webcam + MediaPipe setup, landmark extraction, visualization |
| 2 | Implement geometric heuristics for 4 gestures, temporal smoothing |
| 3 | Add emoji overlay with PIL rendering, debug mode |

**âœ… Deliverable:** Working OpenCV demo with 4 gesture recognition

---

### **Phase 2: Streamlit UI â€” 2 Days**

| Day | Tasks |
|-----|-------|
| 4 | Build Streamlit app structure, webcam integration, basic layout |
| 5 | Add gesture toggle panel, reaction history, settings, polish UI |

**Deliverable:** Interactive web-based demo

---

### **Phase 3: Extended Gestures â€” 2 Days**

| Day | Tasks |
|-----|-------|
| 6 | Implement finger counting + new gesture heuristics (âœŒï¸ ğŸ‘Œ ğŸ‘† âœŠ ğŸ¤˜) |
| 7 | Add gesture toggles to UI, handle conflicts, test all gestures |

**Deliverable:** Expanded gesture library with toggle controls

---

### **Phase 4: Data Collection Tool â€” 2 Days**

| Day | Tasks |
|-----|-------|
| 8 | Build data collection interface with recording, labeling, session management |
| 9 | Add progress tracking, data preview, export functionality |

**Deliverable:** Tool to collect and organize training data

---

### **Phase 5: ML Integration â€” 3 Days**

| Day | Tasks |
|-----|-------|
| 10 | Collect initial dataset (~100 samples per gesture) |
| 11 | Train classifier (SVM/RandomForest), evaluate, iterate |
| 12 | Integrate ML model, add heuristic fallback, final testing |

**Deliverable:** Hybrid heuristic + ML gesture recognition system

---

## **11. Extensions (Optional)**

### **Comparison & Evaluation**
* **Live comparison mode** â€” Side-by-side heuristic vs ML predictions with agreement tracking
* **Accuracy benchmarking** â€” Automated comparison on held-out test set
* **Confusion matrix visualization** â€” Interactive heatmap of misclassifications

### **Additional Gestures**
* **Wave detection** â€” Side-to-side motion with temporal analysis (LSTM/1D CNN)
* **Custom gesture training** â€” Let users define and train their own gestures
* **Two-hand gestures** â€” Heart shape, timeout signal, etc.

### **Audio & Multimodal**
* **Audio-assisted clap detection** â€” Combine visual + audio for robust clapping
* **Voice command integration** â€” "Hey, thumbs up!" triggers gesture mode

### **Deployment & Scale**
* **TensorFlow.js port** â€” Run entirely in browser, no Python backend
* **Mobile app** â€” React Native + MediaPipe for iOS/Android
* **Video conferencing plugin** â€” Zoom/Teams integration for live reactions

### **Advanced CV**
* **Multi-person gesture recognition** â€” Track multiple hands/people simultaneously
* **Full-body gestures** â€” Integrate with OpenPose or Detectron2
* **Depth camera support** â€” Intel RealSense for 3D hand tracking

---

## **12. How to Present This in a Master's Application**

> "I built a real-time gesture recognition system for video-call reactions using MediaPipe hand landmarks. I took a principled engineering approach: starting with heuristic-based detection using geometric rules to recognize core gestures with zero training data, then building an interactive Streamlit UI, expanding to 10+ gestures including finger counting, and finally training a machine learning classifier on custom-collected data for improved robustness. I also built a data collection tool to systematically gather and organize training samples. The project demonstrates my ability to choose the right tool for each problemâ€”simple rules when they suffice, ML when neededâ€”and to build complete end-to-end systems."

This reads extremely strong to admissions committees â€” it shows engineering judgment, full-stack skills, and the ability to iterate on solutions.

---

## **13. Project Status**

### **Phase 1: Core Heuristics â€” Complete âœ…**

- âœ… Webcam + MediaPipe integration
- âœ… Heuristic gesture detection (4 gestures: ğŸ‘ ğŸ‘ âœ‹ ğŸ‘)
- âœ… Temporal smoothing (majority vote)
- âœ… Emoji overlay with PIL rendering
- âœ… Debug mode with landmark visualization

### **Phase 2: Streamlit UI â€” Complete âœ…**

- âœ… Streamlit app structure
- âœ… Live webcam feed in browser
- âœ… Gesture toggle panel
- âœ… Reaction history log
- âœ… Settings panel (detection confidence, smoothing, cooldown)
- âœ… Statistics dashboard (FPS, frame count, gesture counts)

### **Phase 3: Extended Gestures â€” Complete âœ…**

- âœ… Finger counting (1ï¸âƒ£-5ï¸âƒ£)
- âœ… Peace sign (âœŒï¸)
- âœ… OK sign (ğŸ‘Œ)
- âœ… Pointing (ğŸ‘†)
- âœ… Fist (âœŠ)
- âœ… Rock on (ğŸ¤˜)
- âœ… Gesture toggle controls in UI (extended gestures OFF by default)

### **Phase 4: Data Collection Tool â€” Complete âœ…**

- âœ… Recording interface (live webcam with landmark visualization)
- âœ… Label selection system (button per gesture class)
- âœ… Session management (create/load sessions with metadata)
- âœ… Progress tracking (samples per gesture with targets)
- âœ… Data export (combined CSV with normalized landmarks)

### **Phase 5: ML Classifier â€” Complete âœ…**

- âœ… Training UI (`train_model.py`) with Random Forest & SVM
- âœ… Model comparison and evaluation metrics
- âœ… Model export and "Set as Active" workflow
- âœ… Integrated ML classifier into gesture recognition
- âœ… Hybrid mode: ML with heuristic fallback when confidence low
- âœ… ML toggle in Streamlit UI with confidence threshold control

